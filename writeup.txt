
####################

Ajda Gokcen & Kekoa Riggin
LING 567 (S17)
May 12, 2017
Lab 8

####################

##### PART 1 #####
# For whatever you fixed about your grammar as you worked on the test corpus sentence, 
#	provide the usual "phenomenon" write up:
# 	- Prose description of the phenomenon
# 	- IGT (that parses!) illustrating the phenomenon
# 	- Prose description of analysis, illustrated with tdl snippets 
	
	TEST CORPUS SENTENCE

		1.	Description of linguistic facts

		Here is the test corpus sentence we chose to go about parsing:

			### 2 ###
			Source: a
			Vetted: f
			Judgment: g
			Phenomena: {}
			Hoca			çakşırı			denemiş				beğenmemiş
			Hoca			çakşır-ı		dene-miş			beğen-me-miş
			Hodja.3SG.NOM	shawl.3SG-ACC	try-EVPF.NPST.3SG	like-NEG-EVPF.NPST.3SG
			'Hodja tried a shawl and didn't like it.'

		Another little addition: we simply added a few more entries to the lexicon, and last 
		week's changes were enough to allow the following sentence from the test corpus to parse 
		as well, making for three parsing test corpus sentences in total:

			### 4 ###
			Source: a
			Vetted: f
			Judgment: g
			Phenomena: {}
			Aralarında			rekabet				eksik	değil
			Ara-ları-nda		rekabet				eksik	değil
			between-3PLPOSS-LOC	competition.3SG.NOM	lacking	neg.NPST.3SG
			'Competition among them was not lacking.'

		For our test corpus sentence, we needed to cover one phenomenon not already covered by 
		our grammar, and another that needed fixing due to an error in the matrix/customization 
		system... plus some previous errors of our own.

		First: PROPER NOUNS

		Turkish, not unlike English, has a class of proper nouns -- or names -- which do not 
		take determiners and which refer to an entity going by that name.

		Other than that, they act much like common nouns in distribution, with an inherent 3rd 
		person feature, and can take all the same morphology -- case for sure, and also number 
		and possession, though for the latter two it could be debated whether it's the same 
		construction or if the nouns are actually acting as common nouns in those cases.  
		(Let's not get into that.)

		Second: VERB-HEADED COORDINATION

		Coordination in Turkish can be expressed either via asyndeton, where one simply 
		lists all coordinands with no conjunctions between them, or via monosyndeton, where 
		there is a conjunction prior to the final coordinand.  This applies to elements 
		of all types (N, NP, V, VP, S, etc.).

		There is a restriction, however, that either (a) all coordinands have the exact 
		same morphological rules applied to them, or that (b) all non-final coordinands omit 
		the same inflection suffixes across the board (and the values of the missing features 
		are given to be those of the final, obligatorily inflected coordinand).  This latter 
		phenomenon is often referred to as "suspended affixation."

		For verb phrases, if the non-final coordinands omit all inflection, they must take the 
		coordination suffix -ip (and all tense/aspect/mood, negation, and agreement features 
		will be interpreted as equivalent to the inflectionally-supplied features of the final 
		coordinand).

		(It should be noted that we do NOT account for suspended affixation in our grammar.)

		Verbs that require the same object(s) in the same case(s) may be coordinated prior to 
		taking those objects, in addition to more standard VP and S coordination.

		2.	Examples from testsuite

		Here are some (new!) examples of proper names taking case morphology and basically 
		acting a lot like common nouns:

			### PN SUBJ ###
			Source: author
			Vetted: f
			Judgment: g
			Phenomena: {}
			Hoca			kediyi		gördü
			Hoca			kedi-yi		gör-dü
			Hodja.3SG.NOM	cat.3SG-ACC	see-PF.NPST.3SG
			'Hodja saw the cat.'

			### PN OBJ ###
			Source: author
			Vetted: f
			Judgment: g
			Phenomena: {}
			Kadın			Hocayı			gördü
			Kadın			Hoca-yı			gör-dü
			woman.3SG.NOM	Hodja.3SG-ACC	see-PF.NPST.3SG
			'The woman saw Hodja.'

		Here are some examples of VP coordination from the testsuite, which should and do parse:

			### len=2 VP asyndeton (g) ###
			Source: author
			Vetted: f
			Judgment: g
			Phenomena: {coordination}
			Ben				geldim				çıktım
			Ben				gel-di-m			çık-tı-m
			pronoun.1SG.NOM	come-PF.NPST-1SG	exit-PF.NPST-1SG
			'I arrived and left.'

			### len=2 VP mono/polysyndeton (g) ###
			Source: author
			Vetted: f
			Judgment: g
			Phenomena: {coordination}
			Ben				geldim				ve	çıktım
			Ben				gel-di-m			ve	çık-tı-m
			pronoun.1SG.NOM	come-PF.NPST-1SG	and	exit-PF.NPST-1SG
			'I arrived and left.'

		And some examples of suspended affixation, which are still unaccounted for and thus 
		DO NOT PARSE OR GENERATE (here for reference only):

			Source: author
			Vetted: f
			Judgment: g
			Phenomena: {coordination}
			Ben				geldi				ve	çıktım
			Ben				gel-di				ve	çık-tı-m
			pronoun.1SG.NOM	come-PF.NPST		and	exit-PF.NPST-1SG
			'I arrived and left.'

			Source: author
			Vetted: f
			Judgment: g
			Phenomena: {coordination}
			Ben				gelip			çıktım
			Ben				gel-ip			çık-tı-m
			pronoun.1SG.NOM	come-CONV		exit-PF.NPST-1SG
			'I arrived and left.'

		3.	TDL snippets & descriptions

		In order to get proper nouns/names working, we created a new lexical type for them:

			> turkish.tdl

			proper-noun-lex := norm-hook-lex-item & basic-icons-lex-item & number-rule-dtr &
			  [ SYNSEM [ LOCAL [ CAT [ HEAD noun & [ PRON -,
													 MOD < > ],
									   VAL [ SPR < >,
											 SUBJ < >,
											 COMPS < >,
											 SPEC < > ] ],
						 CONT [ HOOK.INDEX.PNG.PER 3rd,
								RELS <! named-relation &
										[ LBL #larg,
										  ARG0 #ind & ref-ind ],
										[ PRED "exist_q_rel",
										  ARG0 #ind,
										  RSTR #harg ] !>,
								HCONS <! [ HARG #harg,
										   LARG #larg ] !> ] ],
						 NON-LOCAL [ SLASH 0-dlist, REL 0-dlist, QUE 0-dlist ] ] ].

		Note that we just let it go SPR-less rather than having it undergo a bare-NP rule; 
		this was a design decision since proper nouns/names don't ever allow determiners as far 
		as we can tell.

		We also included empty NON-LOCAL features in order to block unwanted wh-phrase parses.

		Other than that, all we needed to do was to add the lexical entries for it:

			> lexicon.tdl

			hoca := proper-noun-lex &
			 [ STEM < "Hoca" >,
			   SYNSEM.LKEYS.KEYREL.CARG "Hodja" ].

		This is slightly different from other lexical entries since the predicate is already 
		supplied by inheritance from named-relation, and instead we're supplying the CARG 
		to specify what it is that the entity is named.

		As for verb-headed coordination, there were two things we needed to fix.  First, the 
		corpus sentence wasn't parsing because we were requiring all coordinands to share all 
		head features, which is a problem for some verbal coordination because NEG is a head 
		feature where verbs that have morphological negation are NEG + while all others are 
		NEG -, so negated and non-negated verbs (like those in the test corpus sentence) 
		could not coordinate, which is silly.

		Instead of requiring that all HEAD features are shared across coordinands, we were more 
		specific about which ones matter:

			> turkish.tdl

			bottom-coord-phrase :+
			  [ SYNSEM.LOCAL.CAT.HEAD [ PRON #pron,
										CASE #case,
										CASE-MARKED #cm,
										AUX #aux,
										RAISED #raised ],
				NONCONJ-DTR.SYNSEM.LOCAL.CAT.HEAD [ PRON #pron,
													CASE #case,
													CASE-MARKED #cm,
													AUX #aux,
													RAISED #raised ] ].

			binary-headlike-coord-phrase := coord-phrase & 
			  [ SYNSEM.LOCAL.CAT.HEAD [ PRON #pron,
										CASE #case,
										CASE-MARKED #cm,
										AUX #aux,
										RAISED #raised ],
				LCOORD-DTR.SYNSEM.LOCAL.CAT.HEAD [ PRON #pron,
												   CASE #case,
												   CASE-MARKED #cm,
												   AUX #aux,
												   RAISED #raised ],
				RCOORD-DTR.SYNSEM.LOCAL.CAT.HEAD [ PRON #pron,
												   CASE #case,
												   CASE-MARKED #cm,
												   AUX #aux,
												   RAISED #raised ] ].

			basic-n-top-coord-rule :+ binary-headlike-coord-phrase.
			basic-n-mid-coord-rule :+ binary-headlike-coord-phrase.
			basic-np-top-coord-rule :+ binary-headlike-coord-phrase.
			basic-np-mid-coord-rule :+ binary-headlike-coord-phrase.
			basic-vp-top-coord-rule :+ binary-headlike-coord-phrase.
			basic-vp-mid-coord-rule :+ binary-headlike-coord-phrase.
			basic-s-top-coord-rule :+ binary-headlike-coord-phrase.
			basic-s-mid-coord-rule :+ binary-headlike-coord-phrase.

		... i.e., HEAD.NEG is not one of the HEAD features for which we want to force 
		cross-coordinand equivalence.

		Another thing we needed to fix for verb-headed coordination was out of our hands as 
		far as blame is concerned: here was an issue with matrix.tdl!  Basically, the L-HNDL, 
		L-INDEX, and R-INDEX features of the _and_coord_rel for verbal coordination were all 
		pointing to the right places, but the R-HNDL wasn't pointing to the LBL of the right 
		coordinand as it should.

		We added the following code to fix this problem for verbal asyndeton and monosyndeton, 
		respectively:

			> turkish.tdl

			unary-bottom-coord-rule :+
			  [ C-CONT.HOOK #hook,
				ARGS < [ SYNSEM.LOCAL.CONT.HOOK #hook ] > ].

			binary-bottom-coord-rule :+
			  [ C-CONT.HOOK #hook,
				NONCONJ-DTR.SYNSEM.LOCAL.CONT.HOOK #hook ].

		4.	Problems encountered

		Implementing proper nouns/names was surprisingly straightforward.  Getting verbal 
		coordination to work wasn't so much a hassle as it was something we didn't think could 
		have originated with us, so we had to do the proper authority-check to ensure that 
		matrix.tdl was, indeed, mistaken.

		5.	Generation

		All of the examples shown in part 2 do indeed generate, except for the two noted 
		examples of suspended affixation that (expectedly, though incorrectly) do not parse, 
		of course.

		One observation: the two VP coordination examples generate some sentences where both 
		coordinated VPs are interrogative, i.e. they each contain the interrogative auxiliary.  
		Neat but not good, because the SF should really be shared across coordinands and the 
		parent node, and it's definitely not for these realizations.  A sentence with 
		coordinated [ SF ques ] VPs should not be able to have a top-level node with 
		[ SF prop ] !
		
##### PART 2 #####
# Describe any changes you needed to make the semi.vpm file, and the effects that including 
# the semi.vpm had on generation.
	
	The first change that we made to semi.vpm was ensure that the feature paths are only 
	present on the internal (left-hand side) of each feature.  This means removing PNG and E 
	from the right side as seen in each TDL snippet seen here below.
	
	The English and Chadian Arabic grammars show person with the written form of the ordinal 
	numbers.  So we updated the external values for the PER feature to match those grammars:
		
		> semi.vpm
		
		PNG.PER : PER
		  3rd <> third
		  2nd <> second
		  1st <> first
		  * <> *
	
	In the NUM feature, we made sure (with "* >> *") that the NUM feature is still transferred 
	when underspecified rather than omitted.  We also made all input that's underspecified for 
	number default to singular:
	
		> semi.vpm
		
		PNG.NUM : NUM
		  pl <> pl
		  sg <> sg
		  * >> *
		  sg << *

	Similarly to PER above, the Turkish "nonpast" tense needed to match to "present" tense in 
	English.  We also ensured that underspecified tense was inluded when transfering out.  The 
	last change for tense was defaulting any other incoming tense value to the Turkish nonpast:
	
		> semi.vpm
		
		E.TENSE : TENSE
		  nonpast <> present
		  past <> past
		  * >> *
		  nonpast << [e]
	
	The changes to aspect ensure that underspecified Turkish aspect is transfered out and that 
	any other aspect values (including underspecification) are imported as habitual:
	
		> semi.vpm
		
		E.ASPECT : ASPECT
		  perfective <> perfective
		  imperfective <> imperfective
		  habitual <> habitual
		  prospective <> prospective
		  * >> *
		  habitual << [e]
	
	The only change to the MOOD feature was removing the external feature path from the right 
	side, since we haven't done anything with mood in our grammar:
	
		> semi.vpm
		
		E.MOOD : MOOD
		  * <> *

	We ensured that any value (including an underspecified one) is transfered out.  (This is 
	mostly important for generation; less so for translation, perhaps.)  We also set the 
	default value for EVID in to "direct" (meaning that any underspecified or unfamiliar-valued 
	input value for EVID will be set to "direct"):
	
		> semi.vpm
		
		E.EVID : EVID
		  direct <> direct
		  indirect <> indirect
		  * >> *
		  direct << [e]
	
	Prior to making these changes to semi.vpm, it seems that every parse overgenerated to 
	some extent. After making these changes to semi.vpm, many (happily) generate a smaller 
	number of realizations.  The testsuite examples below are paired with the number of 
	generations (based on their correct parses, of which there should only be one in these 
	cases) from the baseline grammar (Lab 7) and from this week's final grammar:

		Source: author
		Vetted: f
		Judgment: g
		Phenomena: {word order}
		Aile			geldi
		Aile			gel-di
		family.3SG.NOM	come-PF.NPST.3SG
		'The family arrived.'

			Baseline generation:	8
			Final generation:		2

		=====

		Source: author
		Vetted: f
		Judgment: g
		Phenomena: {word order}
		Kediyi		gördü		kadın
		Kedi-yi		gör-dü		kadın
		cat.3SG-ACC	see-PF.NPST.3SG	woman.3SG.NOM
		'The woman saw the cat.'

			Baseline generation:	48
			Final generation: 		6 

		=====

		Source: author
		Vetted: f
		Judgment: g
		Phenomena: {case}
		Ben				kediyle			ilgilendim
		Ben				kedi-yle		ilgilen-di-m
		pronoun.1SG.NOM	cat.3SG-COM		care-PF.NPST-1SG
		'I was interested in the cat.' 

			Baseline generation:	66
			Final generation: 		12

##### PART 3 #####
# Describe any changes you needed to make to get MT working, with "Dogs sleep" or some other 
# simple sentence.

	Apart from this week's instructions regarding variable property mapping, we ran into a 
	couple bumps related to incompatible variable properties.  Although the grammar compiled, we 
	had issues once we actually tried to do the translating.  The variable property mapping 
	for tense when we initially changed it per this lab's instructions looked like this:
	
		> semi.vpm (OLD)
		
		E.TENSE : TENSE
		  nonpast <> nonpast
		  past <> past
		  * <> *
	
	But this mapping gave us a "Problem in create-liszt-fs-from-rels" error at the time of 
	translation.  We first solved this by changing tense to the following, using the English 
	grammar's semi.vpm as reference:

		> semi.vpm (OLD)
		
		E.TENSE : TENSE
		  nonpast <> nonpast
		  past <> past
		  tense <> *

	...which fixed the error, but we still didn't get any parses due to English having a present 
	tense but no nonpast, and vice versa for Turkish.

	And so we changed the code to this:

		> semi.vpm (CURRENT)

		E.TENSE : TENSE
		  nonpast <> present
		  past <> past
		  * >> *
		  nonpast << [e]

	...to ensure that underspecified tense values were transfered out as such from Turkish, that 
	any tense value that did not match nonpast or past was transfered in as nonpast, and that 
	nonpast was transfered out as present.

	The "nonpast <> present" part in particular is very English-specific and may be something 
	we want to move into transfer rules specific to English-Turkish/Turkish-English.

	We needed to add in a few similar changes to aspect, as well:

		> semi.vpm
		
		E.ASPECT : ASPECT
		  perfective <> perfective
		  imperfective <> imperfective
		  habitual <> habitual
		  prospective <> prospective
		  * >> *
		  habitual << [e]

	...with the habitual aspect being the default (since aspect is required for all Turkish 
	verbs) and "* >> *" again ensuring that underspecified features are output as underspecified 
	and not as a lack of that feature entirely, which seemed to happen without that line.

	Outside of semi.vpm, we didn't need to change anything in turkish.tdl, though we adjusted 
	the order of suffixes in irules.tdl in order to make the "nicer" (more common/regular?) 
	suffixes be the ones that appear in generation.

##### PART 4 #####
# Provide the sentence that you worked on for MT, so I can test it.

	The English sentence we used for English-Turkish MT was the following:

		Dogs sleep.

	And the (ideal) Turkish equivalent we used for Turkish-English MT was the following:

		Köpekler uyurlar.

	The Turkish sentence that English-Turkish MT will generate that corresponds to the above 
	(but is unavoidably phonologically-off) looks something like:

		KÖPEKLER UYUIRLER

	...and should be the first in the list of realizations (or at least it is when we run the 
	English-Turkish MT).

	English-Turkish MT for the above English sentence generates 8 Turkish sentences, varying 
	by word order, lexical entries bearing "_dog_n_rel" (of which there are multiple to allow 
	for stem changes in the morphology), and the agreement suffix on the verb (since the two 
	3rd person agreement suffixes for verbs are 3pl and 3-underspecified-for-number, so either 
	is possible).

	Turkish-English MT for the above Turkish sentence generates 2 English sentences:

		the DOGS sleep
		DOGS sleep

	...varying by the COG-ST, since the bare nominative NP "köpekler" is underspecified for 
	COG-ST, and there is no way to make common-noun-headed subject NP explicitly definite.

##### PART 5 #####
# Describe the current coverage of your grammar over your test suite (using numbers you can 
# get from Analyze | Coverage and Analyze | Overgeneration in [incr tsdb()]) and a comparison 
# between your baseline test suite run and your final one for this lab (see Compare | 
# Competence).

	Not too many changes to the stats; just the two proper noun/name examples added to the 
	testsuite, plus more semantics-y fixes and tweaks to make translation work (in addition to 
	the test corpus sentence additions), which didn't affect the test suite performance much.

	BASELINE GRAMMAR TESTSUITE STATS

		Coverage:

			Positive Items #:		121
			Total Results #:		112
			Word String 0:			3.07
			Lexical Items 0:		6.17
			Distinct Analyses 0:	1.62
			Overall Coverage %:		92.6

		Overgeneration:

			Negative Items #:		62
			Total Results #:		10
			Word String 0:			2.85
			Lexical Items 0:		6.30
			Distinct Analyses 0:	2.30
			Overall Coverage %:		16.1

	FINAL GRAMMAR TESTSUITE STATS

		Coverage:

			Positive Items #:		123
			Total Results #:		114
			Word String 0:			3.07
			Lexical Items 0:		6.13
			Distinct Analyses 0:	1.60
			Overall Coverage %:		92.7

		Overgeneration:

			Negative Items #:		62
			Total Results #:		10
			Word String 0:			2.85
			Lexical Items 0:		6.30
			Distinct Analyses 0:	2.30
			Overall Coverage %:		16.1

	FROM COMPARE | COMPETENCE

		Baseline:

			Lexical:				2.02
			Analyses:				1.67
			In:						92.6
			Out:					16.1

		Final:

			Lexical:				2.01
			Analyses:				1.65
			In:						92.7
			Out:					16.1

